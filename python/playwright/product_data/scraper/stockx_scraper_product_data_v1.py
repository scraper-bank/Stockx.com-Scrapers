"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-17
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import asyncio
import json
import logging
import re
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List, Union
from urllib.parse import urlparse, urlencode, urlunparse

from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import stealth_async

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"stockx_com_product_page_scraper_data_{timestamp}.jsonl"

# ScrapeOps Residential Proxy Configuration
PROXY_CONFIG = {
    "server": "http://residential-proxy.scrapeops.io:8181",
    "username": "scrapeops",
    "password": API_KEY
}

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ScrapedData:
    name: str = ""
    name_secondary: str = ""
    brand: str = ""
    category: str = ""
    description: str = ""
    productId: str = ""
    url: str = ""
    currency: str = "USD"
    price: float = 0.0
    preDiscountPrice: Optional[float] = None
    availability: str = "in_stock"
    images: List[Dict[str, str]] = field(default_factory=list)
    features: List[str] = field(default_factory=list)
    reviews: List[Any] = field(default_factory=list)
    specifications: List[Dict[str, str]] = field(default_factory=list)
    serialNumbers: List[Dict[str, str]] = field(default_factory=list)
    variants: List[Dict[str, Any]] = field(default_factory=list)
    videos: List[Any] = field(default_factory=list)
    seller: Dict[str, Any] = field(default_factory=dict)
    aggregateRating: Dict[str, Any] = field(default_factory=dict)
    market_data: Dict[str, Any] = field(default_factory=dict)


class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data: ScrapedData):
        item_key = input_data.url or input_data.name
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found: {item_key}. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")


def strip_html(html_str: str) -> str:
    if not html_str:
        return ""
    clean = re.compile('<[^>]*>')
    return re.sub(clean, ' ', html_str).strip()

def parse_float(price_str: str) -> float:
    if not price_str:
        return 0.0
    match = re.search(r'[\d,]+\.?\d*', price_str)
    if match:
        clean = match.group().replace(',', '')
        try:
            return float(clean)
        except ValueError:
            return 0.0
    return 0.0

def detect_currency(price_text: str) -> str:
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR",
        "GBP": "GBP", "£": "GBP", "GB£": "GBP",
        "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
        "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
        "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
        "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
        "CHF": "CHF", "FR.": "CHF",
        "SEK": "SEK", "KR": "SEK",
        "NZD": "NZD", "NZ$": "NZD",
    }
    for code, currency in currency_map.items():
        if code in price_text:
            return currency
    return "USD"

def make_absolute_url(url_str: str) -> str:
    if not url_str:
        return ""
    if url_str.startswith(("http://", "https://")):
        return url_str
    if url_str.startswith("//"):
        return "https:" + url_str
    domain = "https://stockx.com"
    if url_str.startswith("/"):
        return domain + url_str
    return f"{domain}/{url_str}"

def optimize_url(raw_url: str) -> str:
    try:
        u = urlparse(raw_url)
        q = {
            "fit": "fill",
            "bg": "FFFFFF",
            "w": "1200",
            "h": "857",
            "q": "90",
            "trim": "color"
        }
        # Reconstruct URL with specific query params
        return urlunparse((u.scheme, u.netloc, u.path, u.params, urlencode(q), u.fragment))
    except Exception:
        return raw_url

async def extract_data(page: Page) -> Optional[ScrapedData]:
    """Extract structured data using Playwright."""
    try:
        html_content = await page.content()
        
        # 1. Extract JSON-LD and Next Data
        json_ld_data = {}
        scripts = await page.locator("script[type='application/ld+json']").all_text_contents()
        for script in scripts:
            try:
                data = json.loads(script)
                if isinstance(data, dict) and data.get("@type") == "Product":
                    json_ld_data = data
                    break
            except json.JSONDecodeError:
                continue

        next_data = {}
        next_data_script = await page.locator("script#__NEXT_DATA__").first.text_content()
        if next_data_script:
            try:
                next_data = json.loads(next_data_script)
            except json.JSONDecodeError:
                pass

        # 2. Basic Info Extraction
        brand = ""
        if json_ld_data:
            brand = json_ld_data.get("brand", {}).get("name", "")
        if not brand:
            brand = await page.locator("[data-testid='BreadcrumbWrapper'] a[href*='/brands/']").last.inner_text()
        
        category = ""
        if json_ld_data:
            category = json_ld_data.get("category", "")
        if not category:
            category = await page.locator("[data-testid='BreadcrumbWrapper'] a[href*='/category/']").first.inner_text()

        description = ""
        if json_ld_data:
            description = json_ld_data.get("description", "")
        if not description:
            description = await page.locator("[data-testid='product-description'] p").first.inner_text()

        name = ""
        if json_ld_data:
            name = json_ld_data.get("name", "")
        if not name:
            name = await page.locator("[data-component='primary-product-title']").first.inner_text()

        name_sec = await page.locator("[data-component='secondary-product-title']").first.inner_text()
        
        price_text = await page.locator("[data-testid='trade-box-buy-amount']").first.inner_text()
        price_val = parse_float(price_text)
        currency = detect_currency(price_text)

        # 3. Image Extraction
        result_images = []
        seen_urls = set()

        # Primary Strategy: __NEXT_DATA__
        if next_data:
            try:
                queries = next_data.get("props", {}).get("pageProps", {}).get("states", {}).get("query", {}).get("queries", [])
                for q in queries:
                    product = q.get("state", {}).get("data", {}).get("product")
                    if product and product.get("media") and not product.get("merchandising"):
                        gallery = product.get("media", {}).get("gallery", [])
                        for idx, img_url in enumerate(gallery):
                            if img_url:
                                base_path = img_url.split("?")[0]
                                if base_path not in seen_urls:
                                    seen_urls.add(base_path)
                                    alt = name.strip()
                                    if idx == 1 or "-2" in base_path:
                                        alt += " back view"
                                    result_images.append({
                                        "url": optimize_url(img_url),
                                        "altText": alt
                                    })
                        if result_images:
                            break
            except Exception as e:
                logger.debug(f"Error parsing NextData images: {e}")

        # Fallback Strategy: HTML selectors
        if not result_images:
            imgs = await page.locator("div[data-testid='MultiImage'] [role='tablist'] img").all()
            for i, img_el in enumerate(imgs):
                src = await img_el.get_attribute("src")
                if not src:
                    srcset = await img_el.get_attribute("srcset")
                    if srcset:
                        parts = srcset.split(",")
                        last_part = parts[-1].strip()
                        src = last_part.split(" ")[0]
                
                if src:
                    abs_url = make_absolute_url(src)
                    base_path = abs_url.split("?")[0]
                    if base_path not in seen_urls:
                        seen_urls.add(base_path)
                        alt = name.strip()
                        if i == 1 or "-2" in base_path:
                            alt += " back view"
                        result_images.append({
                            "url": optimize_url(abs_url),
                            "altText": alt
                        })

        # 4. Product ID and Serial Numbers
        pid = json_ld_data.get("sku", "")
        serials = []
        if pid:
            serials.append({"type": "SKU", "value": pid})
        
        gtin = json_ld_data.get("gtin", "")
        if gtin:
            serials.append({"type": "GTIN", "value": gtin})

        traits = await page.locator("[data-component='product-trait']").all()
        specs = []
        for trait in traits:
            key_el = trait.locator("span")
            val_el = trait.locator("p")
            key = (await key_el.inner_text()).strip()
            val = (await val_el.inner_text()).strip()
            if key and val:
                specs.append({"key": key, "value": val})
                if "style" in key.lower():
                    serials.append({"type": "Other", "value": val})

        # 5. Variants
        variants = []
        if json_ld_data and "offers" in json_ld_data:
            offers = json_ld_data["offers"].get("offers", [])
            if isinstance(offers, list):
                for o in offers:
                    v_price = o.get("price")
                    if isinstance(v_price, str):
                        v_price = parse_float(v_price)
                    variants.append({
                        "gtin": o.get("gtin"),
                        "id": o.get("sku"),
                        "price": v_price if v_price else 0.0,
                        "size": o.get("description")
                    })

        # 6. Market Data
        market = {
            "highest_bid": None,
            "last_sale": None,
            "lowest_ask": price_val,
            "sales_last_3_days": None,
        }
        
        chakra_texts = await page.locator("p.chakra-text").all()
        for txt_el in chakra_texts:
            t = await txt_el.inner_text()
            if "Last Sale:" in t:
                # Assuming the value is in the next element or within this one
                sibling = page.locator(f"p.chakra-text:has-text('{t}') + p")
                if await sibling.count() > 0:
                    market["last_sale"] = parse_float(await sibling.first.inner_text())
            if "Sell Now for" in t:
                m = re.search(r'\$([\d,]+)', t)
                if m:
                    market["highest_bid"] = parse_float(m.group(1))

        sold_text = await page.locator("[data-testid='PDPBadge'] h2").inner_text()
        m_sold = re.search(r'(\d+)', sold_text)
        if m_sold:
            market["sales_last_3_days"] = int(m_sold.group(1))

        return ScrapedData(
            name=name.strip(),
            name_secondary=name_sec.strip(),
            brand=brand.strip(),
            category=category.strip(),
            description=description.strip(),
            productId=pid,
            url=make_absolute_url(json_ld_data.get("url", "") or page.url),
            currency=currency,
            price=price_val,
            availability="in_stock",
            images=result_images,
            specifications=specs,
            serialNumbers=serials,
            variants=variants,
            market_data=market,
            seller={
                "name": "StockX",
                "rating": None,
                "url": "https://stockx.com",
            },
            aggregateRating={
                "bestRating": None,
                "ratingValue": None,
                "reviewCount": None,
                "worstRating": None,
            }
        )

    except Exception as e:
        logger.error(f"Error in extract_data: {e}")
        return None


async def scrape_page(browser: Browser, url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    """Scrape a single page with retry logic and performance optimizations."""
    tries = 0
    success = False

    while tries <= retries and not success:
        context = None
        page = None
        try:
            context = await browser.new_context(
                ignore_https_errors=True,
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
            )
            
            page = await context.new_page()
            await stealth_async(page)
            
            async def block_resources(route, request):
                resource_type = request.resource_type
                if resource_type in ["image", "media", "font"]:
                    await route.abort()
                else:
                    await route.continue_()
            
            await page.route("**/*", block_resources)
            
            await page.goto(url, wait_until="domcontentloaded", timeout=180000)
            await page.wait_for_timeout(2000)
            
            scraped_data = await extract_data(page)
            
            if scraped_data:
                pipeline.add_data(scraped_data)
                success = True
            else:
                logger.warning(f"No data extracted from {url}")
        except Exception as e:
            logger.error(f"Exception scraping {url}: {e}")
        finally:
            if page:
                await page.close()
            if context:
                await context.close()
            tries += 1

    if not success:
        logger.error(f"Failed to scrape {url} after {retries} retries.")


async def concurrent_scraping(urls: List[str], max_concurrent: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    """Scrape multiple URLs concurrently with optimizations."""
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            proxy=PROXY_CONFIG,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-accelerated-2d-canvas",
                "--no-first-run",
                "--no-zygote",
                "--disable-gpu",
                "--disable-web-security",
                "--disable-features=IsolateOrigins,site-per-process"
            ]
        )
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def limited_scrape(url):
            async with semaphore:
                await scrape_page(browser, url, pipeline, max_retries)
        
        tasks = [limited_scrape(url) for url in urls]
        await asyncio.gather(*tasks)
        
        await browser.close()


if __name__ == "__main__":
    urls = [
        "https://stockx.com/adidas-ctt-chinese-track-top-31-gender-neutral-jacket-asia-sizing-dark-grey",
    ]

    logger.info("Starting concurrent scraping with Playwright + Stealth...")
    asyncio.run(concurrent_scraping(urls, max_concurrent=1, max_retries=3))
    logger.info("Scraping complete.")