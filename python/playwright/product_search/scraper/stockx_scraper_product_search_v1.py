"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-17
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import asyncio
import json
import logging
import re
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List, Union
from datetime import datetime

from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import stealth_async
from bs4 import BeautifulSoup

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"stockx_com_product_search_page_scraper_data_{timestamp}.jsonl"

# ScrapeOps Residential Proxy Configuration
PROXY_CONFIG = {
    "server": "http://residential-proxy.scrapeops.io:8181",
    "username": "scrapeops",
    "password": API_KEY
}

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ScrapedData:
    breadcrumbs: List[Dict[str, Any]] = field(default_factory=list)
    pagination: Dict[str, Any] = field(default_factory=dict)
    products: List[Dict[str, Any]] = field(default_factory=list)
    recommendations: Dict[str, Any] = field(default_factory=dict)
    relatedSearches: List[str] = field(default_factory=list)
    searchMetadata: Dict[str, Any] = field(default_factory=dict)
    sponsoredProducts: List[Dict[str, Any]] = field(default_factory=list)

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data: Any) -> bool:
        item_key = str(input_data)
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found: {item_key}. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")

def strip_html(h: str) -> str:
    """Removes HTML tags from a string."""
    clean = re.compile('<[^>]*>')
    return re.sub(clean, ' ', h).strip()

def make_absolute_url(url_str: str) -> str:
    """Converts relative URLs to absolute StockX URLs."""
    if not url_str:
        return ""
    if url_str.startswith(("http://", "https://")):
        return url_str
    if url_str.startswith("//"):
        return "https:" + url_str
    domain = "https://stockx.com"
    if url_str.startswith("/"):
        return domain + url_str
    return f"{domain}/{url_str}"

def detect_currency(price_text: str) -> str:
    """Detects currency code from price text."""
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR",
        "GBP": "GBP", "£": "GBP", "GB£": "GBP",
        "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
        "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
        "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
        "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
        "CHF": "CHF", "FR.": "CHF",
        "SEK": "SEK", "KR": "SEK",
        "NZD": "NZD", "NZ$": "NZD",
    }
    for code, currency in currency_map.items():
        if code in price_text:
            return currency
    return "USD"

def clean_price(price_text: str) -> float:
    """Extracts numeric price from string."""
    match = re.search(r'[\d,]+\.?\d*', price_text)
    if match:
        clean = match.group().replace(",", "")
        try:
            return float(clean)
        except ValueError:
            return 0.0
    return 0.0

def find_queries(obj: Any) -> Optional[List[Any]]:
    """Helper to find the queries array anywhere in the JSON tree."""
    if isinstance(obj, list):
        for v in obj:
            res = find_queries(v)
            if res: return res
    elif isinstance(obj, dict):
        if "queries" in obj and isinstance(obj["queries"], list) and len(obj["queries"]) > 0:
            for item in obj["queries"]:
                try:
                    if item.get("state", {}).get("data", {}).get("results"):
                        return obj["queries"]
                except AttributeError:
                    continue
        for v in obj.values():
            res = find_queries(v)
            if res: return res
    return None

async def extract_data(page: Page) -> Optional[ScrapedData]:
    """Extract structured data using Playwright and BeautifulSoup."""
    html = await page.content()
    soup = BeautifulSoup(html, 'html.parser')
    
    output = ScrapedData()
    
    # NEXT_DATA Extraction
    next_data = {}
    next_script = soup.find("script", id="__NEXT_DATA__")
    if next_script:
        try:
            next_data = json.loads(next_script.string)
        except (json.JSONDecodeError, TypeError):
            pass

    # Breadcrumbs
    breadcrumbs = []
    ld_scripts = soup.find_all("script", type="application/ld+json")
    for script in ld_scripts:
        try:
            ld = json.loads(script.string)
            if ld.get("@type") == "BreadcrumbList":
                for item in ld.get("itemListElement", []):
                    inner_item = item.get("item", {})
                    breadcrumbs.append({
                        "name": inner_item.get("name"),
                        "url": inner_item.get("@id")
                    })
        except (json.JSONDecodeError, TypeError):
            continue
    
    if not breadcrumbs:
        for li in soup.select(".chakra-breadcrumb__list-item"):
            a = li.find("a")
            name = li.get_text(strip=True)
            url = make_absolute_url(a['href']) if a and a.has_attr('href') else ""
            breadcrumbs.append({"name": name, "url": url})
    output.breadcrumbs = breadcrumbs

    # Pagination
    pagination = {
        "currentPage": 1,
        "hasNextPage": False,
        "hasPreviousPage": False,
        "nextPageUrl": None,
        "previousPageUrl": None,
        "totalPages": 1,
    }
    p_wrapper = soup.find(attrs={"data-testid": "pagination-wrapper"})
    if p_wrapper:
        for a in p_wrapper.find_all("a"):
            href = a.get("href", "")
            text = a.get_text(strip=True)
            if text.isdigit():
                pagination["totalPages"] = max(pagination["totalPages"], int(text))
            if a.get("aria-label") == "Next":
                pagination["hasNextPage"] = True
                pagination["nextPageUrl"] = make_absolute_url(href)
    output.pagination = pagination

    # Products via JSON logic
    products = []
    queries = find_queries(next_data)
    if queries:
        for q in queries:
            try:
                edges = q.get("state", {}).get("data", {}).get("results", {}).get("edges", [])
                for edge in edges:
                    node = edge.get("node")
                    if not node: continue
                    
                    product_id = node.get("id", "")
                    url_key = node.get("urlKey", "")
                    title = node.get("title", "")
                    brand = node.get("brand", "")
                    category = node.get("productCategory", "")
                    desc = strip_html(node.get("description", "")) if node.get("description") else ""
                    
                    market = node.get("market", {})
                    price_val = market.get("state", {}).get("lowestAsk", {}).get("amount", 0.0)
                    
                    thumb = node.get("media", {}).get("thumbUrl", "")
                    final_image_url = make_absolute_url(thumb)
                    
                    # DOM Supplemental for JSON path
                    badges = []
                    normalized_key = url_key.lstrip("/")
                    link_el = soup.select_one(f"a[href*='{normalized_key}']")
                    if link_el:
                        if "Xpress Ship" in link_el.get_text():
                            badges.append({"label": "Xpress Ship", "type": "limited_time"})
                        if "placeholder" in final_image_url or not final_image_url:
                            img_tag = link_el.find("img")
                            if img_tag and img_tag.get("src"):
                                final_image_url = make_absolute_url(img_tag["src"])
                    
                    variant_size = None
                    if link_el:
                        size_el = link_el.find(attrs={"data-testid": "product-tile-size-display"})
                        if size_el:
                            variant_size = size_el.get_text(strip=True)
                    
                    variants = None
                    if variant_size:
                        variants = {
                            "variantCount": None,
                            "visibleOptions": [{"imageUrl": None, "type": "size", "value": variant_size}]
                        }

                    products.append({
                        "additionalImages": [],
                        "aggregateRating": None,
                        "availability": "in_stock",
                        "badges": badges,
                        "brand": brand,
                        "category": category,
                        "currency": "USD",
                        "description": desc,
                        "features": [],
                        "images": [{"altText": title, "url": final_image_url}],
                        "name": title,
                        "preDiscountPrice": None,
                        "price": price_val,
                        "productId": product_id,
                        "reviews": [],
                        "seller": {"name": "StockX", "rating": None, "url": "https://stockx.com"},
                        "serialNumbers": [],
                        "specifications": [],
                        "url": make_absolute_url("/" + url_key),
                        "variants": variants,
                        "videos": [],
                        "isSponsored": False # Default
                    })
            except Exception as e:
                logger.debug(f"Error parsing product node: {e}")

    # Fallback to DOM Scraping
    if not products:
        tiles = soup.select("[data-testid='ProductTile'], [data-testid='VariantTile']")
        for s in tiles:
            name_el = s.find(attrs={"data-testid": "product-tile-title"})
            name = name_el.get_text(strip=True) if name_el else ""
            if not name:
                p_text = s.select_one("p.chakra-text")
                name = p_text.get_text(strip=True) if p_text else ""
            
            raw_price = ""
            price_el = s.find(attrs={"data-testid": "product-tile-lowest-ask-amount"})
            if price_el:
                raw_price = price_el.get_text(strip=True)
            
            price = clean_price(raw_price)
            img = s.select_one("img.chakra-image")
            img_url = img.get("src", "") if img else ""
            img_alt = img.get("alt", "") if img else ""
            
            link_tag = s.find("a")
            link = link_tag.get("href", "") if link_tag else ""
            
            badges = []
            if "Xpress Ship" in s.get_text():
                badges.append({"label": "Xpress Ship", "type": "limited_time"})
            
            brand = ""
            known_brands = ["New Balance", "Jordan", "Nike", "adidas", "Yeezy", "Asics", "Reebok", "Converse", "Puma"]
            for kb in known_brands:
                if name.lower().startswith(kb.lower()):
                    brand = kb
                    break
            if not brand and name:
                brand = name.split(" ")[0]
            
            is_spon = bool(s.find(attrs={"data-testid": "sponsored-tag"}) or s.has_attr("data-sponsored-listing"))
            
            products.append({
                "additionalImages": [],
                "aggregateRating": None,
                "availability": "in_stock",
                "badges": badges,
                "brand": brand,
                "category": "sneakers",
                "currency": detect_currency(raw_price),
                "description": "",
                "features": [],
                "images": [{"altText": img_alt, "url": make_absolute_url(img_url)}],
                "isSponsored": is_spon,
                "name": name,
                "preDiscountPrice": None,
                "price": price,
                "productId": "",
                "reviews": [],
                "seller": {"name": "StockX", "rating": None, "url": "https://stockx.com"},
                "serialNumbers": [],
                "specifications": [],
                "url": make_absolute_url(link),
                "videos": [],
            })
    
    output.products = products
    output.recommendations = {"relatedProducts": []}
    output.relatedSearches = []
    
    # Metadata
    search_meta = {
        "query": "",
        "resultsDisplayed": len(products),
        "searchType": "keyword",
        "searchUrl": page.url,
        "totalResults": 0,
    }
    search_input = soup.select_one("input#site-search")
    if search_input:
        search_meta["query"] = search_input.get("value", "")
    
    heading = soup.select_one("h1.chakra-heading")
    if heading:
        m = re.search(r'\d+', heading.get_text())
        if m:
            search_meta["totalResults"] = int(m.group())
    output.searchMetadata = search_meta

    # Sponsored
    sponsored = []
    for p in products:
        if p.get("isSponsored"):
            img_info = p.get("images", [{}])[0]
            sponsored.append({
                "currency": p.get("currency"),
                "image": {
                    "altText": p.get("name"),
                    "url": img_info.get("url")
                },
                "name": p.get("name"),
                "price": p.get("price"),
                "sponsorInfo": None,
                "url": p.get("url"),
            })
    output.sponsoredProducts = sponsored
    
    return output

async def scrape_page(browser: Browser, url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    """Scrape a single page with retry logic and performance optimizations."""
    tries = 0
    success = False

    while tries <= retries and not success:
        context = None
        page = None
        try:
            context = await browser.new_context(
                ignore_https_errors=True,
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
            )
            
            page = await context.new_page()
            await stealth_async(page)
            
            # Optimized resource blocking
            async def block_resources(route, request):
                if request.resource_type in ["image", "media", "font"]:
                    await route.abort()
                else:
                    await route.continue_()
            
            await page.route("**/*", block_resources)
            
            await page.goto(url, wait_until="domcontentloaded", timeout=180000)
            await page.wait_for_timeout(2000) # Ensure hydration
            
            scraped_data = await extract_data(page)
            if scraped_data:
                pipeline.add_data(scraped_data)
                success = True
            else:
                logger.warning(f"No data extracted from {url}")
        except Exception as e:
            logger.error(f"Exception scraping {url}: {e}")
        finally:
            if page: await page.close()
            if context: await context.close()
            tries += 1

    if not success:
        logger.error(f"Failed to scrape {url} after {retries} retries.")

async def concurrent_scraping(urls: List[str], max_concurrent: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    """Scrape multiple URLs concurrently with optimizations."""
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            proxy=PROXY_CONFIG,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-accelerated-2d-canvas",
                "--no-first-run",
                "--no-zygote",
                "--disable-gpu",
                "--disable-web-security",
                "--disable-features=IsolateOrigins,site-per-process"
            ]
        )
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def limited_scrape(url):
            async with semaphore:
                await scrape_page(browser, url, pipeline, max_retries)
        
        tasks = [limited_scrape(url) for url in urls]
        await asyncio.gather(*tasks)
        
        await browser.close()

if __name__ == "__main__":
    urls = [
        "https://stockx.com/search?s=kids+shoes",
    ]

    logger.info("Starting concurrent scraping with Playwright + Stealth...")
    asyncio.run(concurrent_scraping(urls, max_concurrent=1, max_retries=3))
    logger.info("Scraping complete.")