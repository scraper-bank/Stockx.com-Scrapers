"""
Generated by: ScrapeOps AI Scraper Generator on 2024-05-22
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import asyncio
import json
import logging
import re
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List
from datetime import datetime

from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import stealth_async

API_KEY = "YOUR-API_KEY"

# ScrapeOps Residential Proxy Configuration
PROXY_CONFIG = {
    "server": "http://residential-proxy.scrapeops.io:8181",
    "username": "scrapeops",
    "password": API_KEY
}

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class ScrapedData:
    appliedFilters: List[Dict[str, str]] = field(default_factory=list)
    bannerImage: str = ""
    breadcrumbs: List[Dict[str, str]] = field(default_factory=list)
    categoryId: str = "electronics"
    categoryName: str = ""
    categoryUrl: str = ""
    description: str = ""
    pagination: Dict[str, Any] = field(default_factory=dict)
    products: List[Dict[str, Any]] = field(default_factory=list)
    subcategories: List[Dict[str, Any]] = field(default_factory=list)

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data: ScrapedData) -> bool:
        # Simple string representation check for duplication
        item_key = hash(json.dumps(asdict(input_data), sort_keys=True))
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found: {item_key}. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")

def strip_html(html_str: str) -> str:
    """Port of Go stripHTML function."""
    if not html_str:
        return ""
    clean = re.sub(r'<[^>]*>', ' ', html_str)
    return clean.strip()

def make_absolute_url(url_str: str) -> str:
    """Port of Go makeAbsoluteURL function."""
    if not url_str:
        return ""
    if url_str.startswith(("http://", "https://")):
        return url_str
    if url_str.startswith("//"):
        return f"https:{url_str}"
    
    domain = "https://stockx.com"
    if url_str.startswith("/"):
        return f"{domain}{url_str}"
    return f"{domain}/{url_str}"

def detect_currency(price_text: str) -> str:
    """Port of Go detectCurrency function."""
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR",
        "GBP": "GBP", "£": "GBP", "GB£": "GBP",
        "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
        "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
        "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
        "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
        "CHF": "CHF", "FR.": "CHF",
        "SEK": "SEK", "KR": "SEK",
        "NZD": "NZD", "NZ$": "NZD",
    }
    for code, currency in currency_map.items():
        if code in price_text:
            return currency
    return "USD"

def parse_price(price_str: str) -> float:
    """Port of Go parsePrice function."""
    match = re.search(r'[\d,]+\.?\d*', price_str)
    if match:
        clean = match.group().replace(",", "")
        try:
            return float(clean)
        except ValueError:
            return 0.0
    return 0.0

async def extract_data(page: Page) -> Optional[ScrapedData]:
    """Extract structured data using Playwright, porting logic from Go ExtractData."""
    try:
        # Extract __NEXT_DATA__
        next_data = {}
        next_data_script = await page.get_attribute("script#__NEXT_DATA__", "innerHTML")
        if next_data_script:
            try:
                next_data = json.loads(next_data_script)
            except json.JSONDecodeError:
                pass

        data = ScrapedData()

        # Applied Filters
        filter_elements = await page.locator(".filter.css-1x2aot1 p.chakra-text").all()
        for el in filter_elements:
            text = await el.text_content()
            data.appliedFilters.append({
                "filterName": "CATEGORY",
                "filterValue": text.strip() if text else ""
            })

        # Banner Image
        banner_meta = await page.get_attribute("meta[property='og:image']", "content")
        data.bannerImage = banner_meta if banner_meta else ""

        # Breadcrumbs
        breadcrumb_elements = await page.locator("nav[data-testid='Breadcrumbs'] li").all()
        for li in breadcrumb_elements:
            name = (await li.text_content() or "").strip()
            anchor = li.locator("a")
            url = ""
            if await anchor.count() > 0:
                href = await anchor.get_attribute("href")
                url = make_absolute_url(href)
            if name:
                data.breadcrumbs.append({"name": name, "url": url})

        # Category info
        h1_text = await page.locator("h1").first.text_content()
        data.categoryName = h1_text.strip() if h1_text else ""
        data.categoryUrl = make_absolute_url("/category/electronics")

        # Description
        desc_elements = await page.locator(".css-1f58kob p").all_text_contents()
        data.description = "\n".join([d.strip() for d in desc_elements if d.strip()])

        # Pagination
        data.pagination = {
            "currentPage": 1,
            "nextPageUrl": None,
            "prevPageUrl": None,
            "resultsPerPage": 40,
            "totalPages": 1,
            "totalResults": 0,
        }
        
        pagination_links = await page.locator("#pagination-control a").all()
        for link in pagination_links:
            aria = await link.get_attribute("aria-label") or ""
            href = await link.get_attribute("href") or ""
            text = await link.text_content() or ""
            
            if "Next" in aria:
                data.pagination["nextPageUrl"] = make_absolute_url(href)
            
            try:
                page_num = int(text.strip())
                if page_num > data.pagination["totalPages"]:
                    data.pagination["totalPages"] = page_num
            except ValueError:
                pass

        # Products
        product_tiles = await page.locator("[data-testid='ProductTile'], [data-component='brand-tile'], [data-testid='VariantTile']").all()
        for tile in product_tiles:
            name_el = tile.locator("[data-testid='product-tile-title']")
            name = await name_el.text_content() if await name_el.count() > 0 else ""
            
            if not name:
                name = await tile.locator("img").first.get_attribute("alt") or ""
            
            if not name:
                continue

            price_text = await tile.locator("[data-testid='product-tile-lowest-ask-amount']").first.text_content() if await tile.locator("[data-testid='product-tile-lowest-ask-amount']").count() > 0 else ""
            link = await tile.locator("a").first.get_attribute("href") or ""
            img_src = await tile.locator("img").first.get_attribute("src") or ""
            is_sponsored = await tile.locator("[data-testid='sponsored-tag']").count() > 0

            brand = name.split(" ")[0] if name else ""
            product_id = ""

            if link:
                link_path = link.split("?")[0]
                parts = [p for p in link_path.strip("/").split("/") if p]
                slug = parts[-1] if parts else ""

                # Strategy 1: DOM Attributes
                pid_attr = await tile.get_attribute("data-product-id")
                if pid_attr:
                    product_id = pid_attr
                else:
                    id_attr = await tile.locator("div[data-testid='productTile']").get_attribute("id")
                    if id_attr:
                        product_id = id_attr

                # Strategy 2: Structured JSON Traversal
                if not product_id and next_data:
                    edges = []
                    try:
                        props = next_data.get("props", {})
                        page_props = props.get("pageProps", {})
                        results = page_props.get("results", {})
                        if "edges" in results:
                            edges = results["edges"]
                        elif "edges" in page_props:
                            edges = page_props["edges"]
                    except Exception:
                        pass

                    for edge in edges:
                        if not isinstance(edge, dict): continue
                        node = edge.get("node", {})
                        url_key = node.get("urlKey") or edge.get("urlKey", "")
                        
                        if url_key == slug and slug:
                            product_id = edge.get("objectId") or node.get("id", "")
                            if product_id: break

                # Strategy 3: Global Regex Fallback
                if not product_id and next_data and slug:
                    raw_str = json.dumps(next_data)
                    slug_pattern = f'"urlKey":"{slug}"'
                    slug_idx = raw_str.find(slug_pattern)
                    if slug_idx != -1:
                        id_regex = re.compile(r'[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}')
                        search_start = max(0, slug_idx - 500)
                        search_window = raw_str[search_start : slug_idx + 200]
                        matches = id_regex.findall(search_window)
                        if matches:
                            product_id = matches[-1]

                # Strategy 4: Final Fallback
                if not product_id:
                    product_id = slug

            data.products.append({
                "availability": "in_stock",
                "brand": brand,
                "currency": detect_currency(price_text),
                "image": make_absolute_url(img_src),
                "isPrime": False,
                "isSponsored": is_sponsored,
                "name": name.strip(),
                "preDiscountPrice": None,
                "price": parse_price(price_text),
                "productId": product_id,
                "rating": None,
                "reviewCount": None,
                "url": make_absolute_url(link),
            })

        # Subcategories
        subcat_elements = await page.locator(".filter.css-qlswoo a").all()
        for sc in subcat_elements:
            href = await sc.get_attribute("href") or ""
            name = await sc.text_content() or ""
            data.subcategories.append({
                "name": name.strip(),
                "productCount": None,
                "url": make_absolute_url(href),
            })

        return data
    except Exception as e:
        logger.error(f"Error extracting data: {e}")
        return None

async def scrape_page(browser: Browser, url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    """Scrape a single page with retry logic and performance optimizations."""
    tries = 0
    success = False

    while tries <= retries and not success:
        context = None
        page = None
        try:
            context = await browser.new_context(
                ignore_https_errors=True,
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
            )
            
            page = await context.new_page()
            await stealth_async(page)
            
            async def block_resources(route, request):
                if request.resource_type in ["image", "media", "font"]:
                    await route.abort()
                else:
                    await route.continue_()
            
            await page.route("**/*", block_resources)
            
            response = await page.goto(url, wait_until="domcontentloaded", timeout=180000)
            if response.status >= 400:
                logger.warning(f"Received status {response.status} for {url}")
            
            await page.wait_for_timeout(2000)
            
            scraped_data = await extract_data(page)
            
            if scraped_data and (scraped_data.products or scraped_data.categoryName):
                pipeline.add_data(scraped_data)
                success = True
            else:
                logger.warning(f"No meaningful data extracted from {url}, retry {tries}")
        except Exception as e:
            logger.error(f"Exception scraping {url}: {e}")
        finally:
            if page: await page.close()
            if context: await context.close()
            tries += 1

    if not success:
        logger.error(f"Failed to scrape {url} after {retries} retries.")

def generate_output_filename() -> str:
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"stockx_com_product_category_page_scraper_data_{timestamp}.jsonl"

async def concurrent_scraping(urls: List[str], max_concurrent: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    """Scrape multiple URLs concurrently with optimizations."""
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            proxy=PROXY_CONFIG,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-accelerated-2d-canvas",
                "--no-first-run",
                "--no-zygote",
                "--disable-gpu",
                "--disable-web-security",
                "--disable-features=IsolateOrigins,site-per-process"
            ]
        )
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def limited_scrape(url):
            async with semaphore:
                await scrape_page(browser, url, pipeline, max_retries)
        
        tasks = [limited_scrape(url) for url in urls]
        await asyncio.gather(*tasks)
        await browser.close()

if __name__ == "__main__":
    urls = [
        "https://stockx.com/category/electronics",
    ]

    logger.info("Starting concurrent scraping with Playwright + Stealth...")
    asyncio.run(concurrent_scraping(urls, max_concurrent=1, max_retries=3))
    logger.info("Scraping complete.")