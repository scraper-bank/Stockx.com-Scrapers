"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-17
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import undetected_chromedriver as uc
from seleniumwire import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException
import json
import re
from concurrent.futures import ThreadPoolExecutor
import logging
from dataclasses import dataclass, field
from typing import Dict, Any, Optional, List
import threading

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"stockx_com_product_category_page_scraper_data_{timestamp}.jsonl"

# ScrapeOps Residential Proxy Configuration
PROXY_CONFIG = {
    'proxy': {
        'http': f'http://scrapeops:{API_KEY}@residential-proxy.scrapeops.io:8181',
        'https': f'http://scrapeops:{API_KEY}@residential-proxy.scrapeops.io:8181',
        'no_proxy': 'localhost:127.0.0.1'
    }
}

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Thread-local storage for WebDriver instances
thread_local = threading.local()

@dataclass
class ScrapedData:
    appliedFilters: List[str] = field(default_factory=list)
    bannerImage: str = ""
    breadcrumbs: List[Dict[str, str]] = field(default_factory=list)
    categoryId: str = "electronics"
    categoryName: str = ""
    categoryUrl: str = ""
    description: str = ""
    pagination: Dict[str, Any] = field(default_factory=dict)
    products: List[Dict[str, Any]] = field(default_factory=list)
    subcategories: List[Dict[str, Any]] = field(default_factory=list)

    def to_dict(self):
        return {k: v for k, v in self.__dict__.items()}

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data):
        # Create a stable key from product IDs to avoid duplicates in the same run
        if isinstance(input_data, ScrapedData) and input_data.products:
            item_key = "-".join([str(p.get('productId')) for p in input_data.products[:5]])
        else:
            item_key = str(input_data)
        
        if item_key in self.items_seen:
            logger.warning(f"Duplicate content batch found. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(scraped_data.to_dict(), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")

def get_driver():
    """Get thread-local undetected ChromeDriver instance with ScrapeOps Residential Proxy."""
    if not hasattr(thread_local, "driver"):
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.default_content_setting_values.notifications": 2,
            "profile.managed_default_content_settings.cookies": 1,
            "profile.managed_default_content_settings.javascript": 1,
        }
        
        options = uc.ChromeOptions()
        options.add_argument("--headless=new")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--window-size=1920,1080")
        
        options.add_experimental_option("prefs", prefs)
        options.page_load_strategy = 'eager'
        
        thread_local.driver = webdriver.Chrome(
            options=options,
            seleniumwire_options=PROXY_CONFIG
        )
        
    return thread_local.driver

def strip_html(html_str: str) -> str:
    if not html_str:
        return ""
    clean = re.compile('<[^>]*>')
    return re.sub(clean, ' ', html_str).strip()

def make_absolute_url(url_str: str) -> str:
    if not url_str:
        return ""
    if url_str.startswith(("http://", "https://")):
        return url_str
    if url_str.startswith("//"):
        return "https:" + url_str
    domain = "https://stockx.com"
    if url_str.startswith("/"):
        return domain + url_str
    return f"{domain}/{url_str}"

def extract_numeric_price(price_text: str) -> float:
    if not price_text:
        return 0.0
    match = re.search(r'[\d,]+\.?\d*', price_text)
    if match:
        clean = match.group().replace(",", "")
        try:
            return float(clean)
        except ValueError:
            return 0.0
    return 0.0

def extract_data(driver: webdriver.Chrome, url: str) -> Optional[ScrapedData]:
    """Extracted data logic converted from Go ExtractData."""
    try:
        # 1. Extract NEXT_DATA for primary products
        next_data = {}
        try:
            script_tag = driver.find_element(By.ID, "__NEXT_DATA__")
            next_data = json.loads(script_tag.get_attribute('innerHTML'))
        except NoSuchElementException:
            pass

        # 2. Breadcrumbs extraction (JSON-LD fallback to DOM)
        breadcrumbs = []
        ld_scripts = driver.find_elements(By.XPATH, "//script[@type='application/ld+json']")
        for script in ld_scripts:
            try:
                data = json.loads(script.get_attribute('innerHTML'))
                if data.get("@type") == "BreadcrumbList":
                    for item in data.get("itemListElement", []):
                        if "item" in item:
                            name = item["item"].get("name", "")
                            item_id = item["item"].get("@id", "")
                            if name:
                                breadcrumbs.append({
                                    "name": name,
                                    "url": make_absolute_url(item_id)
                                })
            except (json.JSONDecodeError, KeyError):
                continue

        if not breadcrumbs:
            bc_items = driver.find_elements(By.CLASS_NAME, "chakra-breadcrumb__list-item")
            for item in bc_items:
                try:
                    link = item.find_element(By.TAG_NAME, "a")
                    name = link.text.strip() or item.text.strip()
                    href = link.get_attribute("href")
                    if name:
                        breadcrumbs.append({"name": name, "url": make_absolute_url(href)})
                except NoSuchElementException:
                    continue

        # 3. Meta and Header fields
        banner_image = ""
        try:
            banner_image = driver.find_element(By.XPATH, "//meta[@property='og:image']").get_attribute("content")
        except NoSuchElementException:
            pass

        description = ""
        try:
            description = driver.find_element(By.CSS_SELECTOR, ".chakra-text.css-1o63kog").text.strip()
        except NoSuchElementException:
            pass

        category_name = ""
        try:
            category_name = driver.find_element(By.CSS_SELECTOR, "h1.chakra-heading").text.strip()
        except NoSuchElementException:
            pass

        # 4. Product Extraction logic
        products = []
        
        # Try NEXT_DATA parsing first
        try:
            edges = next_data.get("props", {}).get("pageProps", {}).get("results", {}).get("edges", [])
            for edge in edges:
                node = edge.get("node", {})
                title = node.get("title", "")
                p = {
                    "availability": "in_stock",
                    "name": title,
                    "brand": node.get("brand") or (title.split(" ")[0] if title else ""),
                    "currency": "USD",
                    "image": node.get("media", {}).get("thumbUrl", ""),
                    "isPrime": False,
                    "isSponsored": False,
                    "preDiscountPrice": None,
                    "price": node.get("market", {}).get("state", {}).get("lowestAsk", {}).get("amount", 0.0),
                    "productId": node.get("id") or edge.get("objectId", ""),
                    "rating": None,
                    "reviewCount": None,
                    "url": make_absolute_url("/" + node.get("urlKey", "")) if node.get("urlKey") else ""
                }
                products.append(p)
        except (AttributeError, KeyError):
            pass

        # Fallback to DOM parsing if NEXT_DATA failed or returned no products
        if not products:
            tiles = driver.find_elements(By.CSS_SELECTOR, "[data-testid='ProductTile'], [data-testid='VariantTile']")
            for tile in tiles:
                try:
                    name = tile.find_element(By.CSS_SELECTOR, "[data-testid='product-tile-title']").text.strip()
                    price_str = tile.find_element(By.CSS_SELECTOR, "[data-testid='product-tile-lowest-ask-amount']").text
                    link_el = tile.find_element(By.TAG_NAME, "a")
                    href = link_el.get_attribute("href")
                    
                    p = {
                        "availability": "in_stock",
                        "currency": "USD",
                        "image": tile.find_element(By.TAG_NAME, "img").get_attribute("src"),
                        "isPrime": False,
                        "isSponsored": len(tile.find_elements(By.CSS_SELECTOR, "[data-testid='sponsored-tag']")) > 0,
                        "name": name,
                        "brand": name.split(" ")[0] if name else "",
                        "preDiscountPrice": None,
                        "price": extract_numeric_price(price_str),
                        "url": make_absolute_url(href),
                        "productId": href.split("/")[-1] if href else "",
                        "rating": None,
                        "reviewCount": None
                    }
                    products.append(p)
                except (NoSuchElementException, StaleElementReferenceException):
                    continue

        # 5. Subcategories extraction
        subcategories = []
        sub_els = driver.find_elements(By.CSS_SELECTOR, ".filter.css-1x2aot1 a")
        for sub in sub_els:
            try:
                sub_name = sub.text.strip()
                sub_href = sub.get_attribute("href")
                if sub_name:
                    subcategories.append({
                        "name": sub_name,
                        "productCount": 0,
                        "url": make_absolute_url(sub_href)
                    })
            except (NoSuchElementException, StaleElementReferenceException):
                continue

        return ScrapedData(
            appliedFilters=[],
            bannerImage=banner_image or "",
            breadcrumbs=breadcrumbs,
            categoryId="electronics",
            categoryName=category_name,
            categoryUrl=make_absolute_url("/category/electronics"),
            description=description,
            pagination={
                "currentPage": 1,
                "nextPageUrl": make_absolute_url("/category/electronics?page=2"),
                "prevPageUrl": None,
                "resultsPerPage": 40,
                "totalPages": 25,
                "totalResults": 1000,
            },
            products=products,
            subcategories=subcategories
        )
    except Exception as e:
        logger.error(f"Error extracting data: {e}")
        return None

def scrape_page(url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    """Scrape a single page with retry logic."""
    tries = 0
    success = False
    driver = get_driver()

    while tries <= retries and not success:
        try:
            driver.get(url)
            
            # Explicit wait for product tiles or the NEXT_DATA script
            WebDriverWait(driver, 15).until(
                lambda d: d.find_elements(By.CSS_SELECTOR, "[data-testid='ProductTile']") or 
                          d.find_elements(By.ID, "__NEXT_DATA__")
            )
            
            scraped_data = extract_data(driver, url)
            
            if scraped_data:
                pipeline.add_data(scraped_data)
                success = True
            else:
                logger.warning(f"No data extracted from {url}")
        except Exception as e:
            logger.error(f"Attempt {tries+1} failed for {url}: {e}")
            # Refresh driver on consecutive failures
            if tries == retries - 1:
                driver.quit()
                if hasattr(thread_local, "driver"):
                    del thread_local.driver
                driver = get_driver()
        finally:
            tries += 1

    if not success:
        logger.error(f"Failed to scrape {url} after {retries} retries.")

def concurrent_scraping(urls: List[str], max_threads: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    """Scrape multiple URLs concurrently."""
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    
    try:
        with ThreadPoolExecutor(max_workers=max_threads) as executor:
            futures = [executor.submit(scrape_page, url, pipeline, max_retries) for url in urls]
            for future in futures:
                future.result()
    finally:
        # Clean up drivers for all threads
        if hasattr(thread_local, "driver"):
            try:
                thread_local.driver.quit()
            except:
                pass

if __name__ == "__main__":
    urls = [
        "https://stockx.com/category/electronics",
    ]

    logger.info("Starting concurrent scraping with undetected ChromeDriver + Selenium Wire...")
    concurrent_scraping(urls, max_threads=1, max_retries=3)
    logger.info("Scraping complete.")