import undetected_chromedriver as uc
from seleniumwire import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import json
import re
from concurrent.futures import ThreadPoolExecutor
import logging
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List, Union
import threading
from urllib.parse import urljoin

"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-17
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"stockx_com_product_page_scraper_data_{timestamp}.jsonl"

# ScrapeOps Residential Proxy Configuration
PROXY_CONFIG = {
    'proxy': {
        'http': f'http://scrapeops:{API_KEY}@residential-proxy.scrapeops.io:8181',
        'https': f'http://scrapeops:{API_KEY}@residential-proxy.scrapeops.io:8181',
        'no_proxy': 'localhost:127.0.0.1'
    }
}

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Thread-local storage for WebDriver instances
thread_local = threading.local()

@dataclass
class ScrapedData:
    name: str = ""
    productId: str = ""
    brand: str = ""
    category: str = ""
    price: float = 0.0
    currency: str = "USD"
    description: str = ""
    availability: str = "in_stock"
    url: str = ""
    images: List[Dict[str, str]] = field(default_factory=list)
    specifications: List[Dict[str, str]] = field(default_factory=list)
    serialNumbers: List[Dict[str, str]] = field(default_factory=list)
    seller: Dict[str, Any] = field(default_factory=lambda: {
        "name": "StockX",
        "rating": None,
        "url": "https://stockx.com"
    })
    aggregateRating: Optional[Any] = None
    preDiscountPrice: Optional[Any] = None
    features: List[str] = field(default_factory=list)
    reviews: List[Any] = field(default_factory=list)
    videos: List[Any] = field(default_factory=list)

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data):
        item_key = input_data.productId if hasattr(input_data, 'productId') else str(input_data)
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found: {item_key}. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")

def get_driver():
    """Get thread-local undetected ChromeDriver instance with ScrapeOps Residential Proxy."""
    if not hasattr(thread_local, "driver"):
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.default_content_setting_values.notifications": 2,
            "profile.managed_default_content_settings.stylesheets": 2,
            "profile.managed_default_content_settings.cookies": 1,
            "profile.managed_default_content_settings.javascript": 1,
        }
        
        options = uc.ChromeOptions()
        options.add_argument("--headless=new")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--window-size=1920,1080")
        
        options.add_experimental_option("prefs", prefs)
        options.page_load_strategy = 'eager'
        
        thread_local.driver = webdriver.Chrome(
            options=options,
            seleniumwire_options=PROXY_CONFIG,
            version_main=None
        )
    return thread_local.driver

def detect_currency(price_text: str) -> str:
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR",
        "GBP": "GBP", "£": "GBP", "GB£": "GBP",
        "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
        "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
        "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
        "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
        "CHF": "CHF", "FR.": "CHF",
        "SEK": "SEK", "KR": "SEK",
        "NZD": "NZD", "NZ$": "NZD",
    }
    for code, currency in currency_map.items():
        if code in price_text:
            return currency
    return "USD"

def optimize_img_url(u: str) -> str:
    if "stockx.com/images" not in u:
        return u
    base = u.split("?")[0]
    return f"{base}?fit=fill&bg=FFFFFF&w=1200&h=857&q=100"

def extract_data(driver: webdriver.Chrome, page_url: str) -> Optional[ScrapedData]:
    """Ported ExtractData logic from Go to Python using Selenium and DOM data."""
    try:
        # 1. Parse JSON data from scripts
        next_data = {}
        try:
            script_tag = driver.find_element(By.CSS_SELECTOR, "script#__NEXT_DATA__")
            next_data = json.loads(script_tag.get_attribute("textContent"))
        except (NoSuchElementException, json.JSONDecodeError):
            pass

        product_schema = {}
        try:
            schema_tag = driver.find_element(By.CSS_SELECTOR, "script[data-testid='product-schema']")
            product_schema = json.loads(schema_tag.get_attribute("textContent"))
        except (NoSuchElementException, json.JSONDecodeError):
            pass

        # Depth-access product data from __NEXT_DATA__
        product_data = {}
        try:
            queries = next_data.get("props", {}).get("pageProps", {}).get("states", {}).get("query", {}).get("value", {}).get("queries", [])
            if queries:
                product_data = queries[0].get("state", {}).get("data", {}).get("product", {})
        except (KeyError, IndexError):
            pass

        # 2. Extract Fields
        name = product_data.get("title") or product_schema.get("name")
        if not name:
            try:
                name = driver.find_element(By.CSS_SELECTOR, "[data-component='primary-product-title']").text
            except NoSuchElementException:
                name = ""

        brand = product_data.get("brand") or product_schema.get("brand", {}).get("name")
        if not brand:
            try:
                breadcrumbs = driver.find_elements(By.CSS_SELECTOR, ".chakra-breadcrumb__list-item")
                if breadcrumbs:
                    brand = breadcrumbs[-1].text
            except (NoSuchElementException, IndexError):
                brand = ""

        availability = "in_stock"
        schema_avail = product_schema.get("offers", {}).get("availability", "")
        if schema_avail and "InStock" not in schema_avail:
            availability = "out_of_stock"

        price = 0.0
        offers = product_schema.get("offers", {})
        low_price = offers.get("lowPrice")
        if low_price:
            try:
                price = float(str(low_price).replace(",", ""))
            except ValueError:
                pass
        
        if price == 0:
            try:
                price_text = driver.find_element(By.CSS_SELECTOR, "[data-testid='trade-box-buy-amount']").text
                match = re.search(r"[\d,.]+", price_text)
                if match:
                    price = float(match.group().replace(",", ""))
            except (NoSuchElementException, ValueError):
                pass

        currency = offers.get("priceCurrency")
        if not currency:
            try:
                price_text = driver.find_element(By.CSS_SELECTOR, "[data-testid='trade-box-buy-amount']").text
                currency = detect_currency(price_text)
            except NoSuchElementException:
                currency = "USD"

        description = product_data.get("description") or product_schema.get("description")
        if not description:
            try:
                description = driver.find_element(By.CSS_SELECTOR, "[data-testid='product-description']").text
            except NoSuchElementException:
                description = ""
        description = description.replace("<br>", "\n").strip()

        # Images
        images = []
        seen_images = set()
        product_title = name.strip()

        # JSON extraction
        gallery = product_data.get("media", {}).get("gallery", [])
        for img_url in gallery:
            if isinstance(img_url, str):
                high_res = optimize_img_url(img_url)
                base_path = high_res.split("?")[0]
                if base_path not in seen_images:
                    alt = product_title
                    if "-2.jpg" in base_path or "_2.jpg" in base_path:
                        alt += " back view"
                    images.append({"alt_text": alt, "url": high_res})
                    seen_images.add(base_path)

        # Fallback HTML extraction
        img_elements = driver.find_elements(By.CSS_SELECTOR, "[data-testid='MultiImage'] img, .chakra-tabs__tab img")
        for img in img_elements:
            try:
                src = img.get_attribute("src")
                if not src or "Placeholder" in src or "tracking" in src:
                    continue
                high_res = optimize_img_url(src)
                base_path = high_res.split("?")[0]
                if base_path not in seen_images:
                    alt = img.get_attribute("alt") or product_title
                    if alt == "nextProductImage":
                        alt = product_title
                    if ("-2.jpg" in base_path or "_2.jpg" in base_path) and "back view" not in alt.lower():
                        alt += " back view"
                    images.append({"alt_text": alt.strip(), "url": high_res})
                    seen_images.add(base_path)
            except Exception:
                continue

        # Specifications (Traits)
        specifications = []
        traits = product_data.get("traits", [])
        for trait in traits:
            if isinstance(trait, dict):
                specifications.append({
                    "key": str(trait.get("name", "")),
                    "value": str(trait.get("value", ""))
                })
        
        if not specifications:
            trait_elms = driver.find_elements(By.CSS_SELECTOR, "[data-component='product-trait']")
            for el in trait_elms:
                try:
                    key = el.find_element(By.TAG_NAME, "span").text.strip()
                    val = el.find_element(By.TAG_NAME, "p").text.strip()
                    if key and val:
                        specifications.append({"key": key, "value": val})
                except NoSuchElementException:
                    continue

        # Serial Numbers
        sku_val = product_schema.get("sku", "")
        gtin_val = product_schema.get("gtin", "")
        style_val = product_data.get("styleId", "")
        if not style_val:
            for spec in specifications:
                if spec["key"].lower() == "style":
                    style_val = spec["value"]
                    break

        serial_numbers = []
        seen_serials = set()
        if sku_val:
            serial_numbers.append({"type": "SKU", "value": sku_val})
            seen_serials.add(sku_val)
        if gtin_val and gtin_val not in seen_serials:
            serial_numbers.append({"type": "GTIN", "value": gtin_val})
            seen_serials.add(gtin_val)
        if style_val and style_val not in seen_serials:
            serial_numbers.append({"type": "Other", "value": style_val})

        # Final URL
        canonical = product_schema.get("url")
        if not canonical:
            try:
                canonical = driver.find_element(By.CSS_SELECTOR, "link[rel='canonical']").get_attribute("href")
            except NoSuchElementException:
                canonical = page_url
        
        final_url = urljoin("https://stockx.com", canonical) if canonical else page_url

        return ScrapedData(
            name=name.strip(),
            productId=product_data.get("id") or sku_val or "",
            brand=brand,
            category=product_data.get("productCategory") or product_schema.get("category", ""),
            price=price,
            currency=currency,
            description=description,
            availability=availability,
            url=final_url,
            images=images,
            specifications=specifications,
            serialNumbers=serial_numbers
        )
    except Exception as e:
        logger.error(f"Extraction error: {e}")
        return None

def scrape_page(url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    tries = 0
    success = False
    driver = get_driver()

    while tries <= retries and not success:
        try:
            driver.get(url)
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "body"))
            )
            
            scraped_data = extract_data(driver, url)
            if scraped_data:
                pipeline.add_data(scraped_data)
                success = True
            else:
                logger.warning(f"No data extracted from {url}")
        except Exception as e:
            logger.error(f"Exception scraping {url}: {e}")
        finally:
            tries += 1

    if not success:
        logger.error(f"Failed to scrape {url} after {retries} retries.")

def concurrent_scraping(urls: List[str], max_threads: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = [executor.submit(scrape_page, url, pipeline, max_retries) for url in urls]
        for future in futures:
            future.result()
    
    if hasattr(thread_local, "driver"):
        thread_local.driver.quit()

if __name__ == "__main__":
    urls = [
        "https://stockx.com/adidas-ctt-chinese-track-top-31-gender-neutral-jacket-asia-sizing-dark-grey",
    ]

    logger.info("Starting concurrent scraping with undetected ChromeDriver + Selenium Wire...")
    concurrent_scraping(urls, max_threads=1, max_retries=3)
    logger.info("Scraping complete.")