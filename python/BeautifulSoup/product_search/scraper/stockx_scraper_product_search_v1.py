"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-17
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urlencode, urljoin
import json
import re
import logging
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List, Union

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"stockx_com_product_search_page_scraper_data_{timestamp}.jsonl"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ScrapedData:
    breadcrumbs: List[Dict[str, str]] = field(default_factory=list)
    pagination: Dict[str, Any] = field(default_factory=lambda: {"currentPage": 1, "totalPages": 1})
    products: List[Dict[str, Any]] = field(default_factory=list)
    searchMetadata: Dict[str, Any] = field(default_factory=dict)

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data: ScrapedData) -> bool:
        # Using products list as a marker for uniqueness check in this context
        item_key = str([p.get("productId") or p.get("url") for p in input_data.products])
        if item_key in self.items_seen:
            logger.warning(f"Duplicate data set found. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved items to {self.jsonl_filename}")

def strip_html(html_str: str) -> str:
    """Removes HTML tags from a string."""
    if not html_str:
        return ""
    clean = re.compile('<[^>]*>')
    return re.sub(clean, ' ', html_str).strip()

def make_absolute_url(url_str: str) -> str:
    """Converts relative URL to absolute URL."""
    if not url_str:
        return ""
    if url_str.startswith(("http://", "https://")):
        return url_str
    if url_str.startswith("//"):
        return "https:" + url_str
    domain = "https://stockx.com"
    if url_str.startswith("/"):
        return domain + url_str
    return domain + "/" + url_str

def detect_currency(price_text: str) -> str:
    """Detects currency code from price string."""
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR",
        "GBP": "GBP", "£": "GBP", "GB£": "GBP",
        "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
        "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
        "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
        "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
        "CHF": "CHF", "FR.": "CHF",
        "SEK": "SEK", "KR": "SEK",
        "NZD": "NZD", "NZ$": "NZD",
    }
    for code, currency in currency_map.items():
        if code in price_text:
            return currency
    return "USD"

def extract_price_value(price_text: str) -> float:
    """Extracts numerical price value from string."""
    if not price_text:
        return 0.0
    clean_text = price_text.replace(",", "")
    match = re.search(r'[\d,]+\.?\d*', clean_text)
    if match:
        try:
            return float(match.group().replace(",", ""))
        except ValueError:
            return 0.0
    return 0.0

def extract_data(soup: BeautifulSoup) -> Optional[ScrapedData]:
    """Extract structured data from HTML using BeautifulSoup."""
    products = []
    
    # 1. Attempt Extraction from __NEXT_DATA__ JSON
    next_data_script = soup.find("script", id="__NEXT_DATA__")
    json_success = False
    
    if next_data_script:
        try:
            data = json.loads(next_data_script.string)
            # Drill down into the specific StockX nested structure
            # Logic based on Go implementation: props.pageProps.states.query.value.queries[1].state.data.results.edges
            queries = data.get("props", {}).get("pageProps", {}).get("states", {}).get("query", {}).get("value", {}).get("queries", [])
            
            if len(queries) >= 2:
                results_data = queries[1].get("state", {}).get("data", {}).get("results", {})
                edges = results_data.get("edges", [])
                
                for edge in edges:
                    node = edge.get("node", {})
                    url_key = node.get("urlKey", "")
                    
                    p = {
                        "productId": node.get("id"),
                        "name": node.get("title"),
                        "brand": node.get("brand"),
                        "description": node.get("description"),
                        "url": make_absolute_url("/" + url_key),
                        "category": node.get("productCategory"),
                        "price": node.get("market", {}).get("state", {}).get("lowestAsk", {}).get("amount", 0.0),
                        "currency": "USD",
                        "availability": "in_stock",
                        "images": [{"url": f"https://images.stockx.com/images/{url_key}-Product.jpg", "altText": node.get("title")}],
                        "features": []
                    }
                    
                    if node.get("model"):
                        p["features"].append(f"Model: {node['model']}")
                    if node.get("condition"):
                        p["features"].append(f"Condition: {node['condition']}")
                    
                    for trait in node.get("traits", []):
                        name = trait.get("name")
                        val = trait.get("value")
                        if name and val:
                            p["features"].append(f"{name}: {val}")
                            
                    products.append(p)
                
                if products:
                    json_success = True
        except (json.JSONDecodeError, KeyError, IndexError, TypeError) as e:
            logger.debug(f"JSON extraction failed: {e}")

    # 2. Fallback to DOM Scraping
    if not json_success:
        product_tiles = soup.select("[data-testid='ProductTile']")
        for tile in product_tiles:
            p = {}
            name_el = tile.select_one("[data-testid='product-tile-title']")
            if not name_el:
                name_el = tile.select_one("p.chakra-text")
            p["name"] = name_el.get_text(strip=True) if name_el else ""

            price_el = tile.select_one("[data-testid='product-tile-lowest-ask-amount']")
            price_text = price_el.get_text(strip=True) if price_el else ""
            p["price"] = extract_price_value(price_text)
            p["currency"] = detect_currency(price_text)

            link_el = tile.find("a")
            p["url"] = make_absolute_url(link_el.get("href", "")) if link_el else ""

            img_el = tile.find("img")
            if img_el:
                p["images"] = [{"url": img_el.get("src", ""), "altText": img_el.get("alt", "")}]
            else:
                p["images"] = []

            p["availability"] = "in_stock"
            products.append(p)

    # 3. Breadcrumbs
    breadcrumbs = []
    for item in soup.select(".chakra-breadcrumb__list-item"):
        link = item.select_one("a, span")
        text = link.get_text(strip=True) if link else ""
        href = item.select_one("a")
        url = make_absolute_url(href.get("href", "")) if href else ""
        if text and text != "/":
            breadcrumbs.append({"name": text, "url": url})

    # 4. Pagination
    pagination = {"currentPage": 1, "totalPages": 1}
    # Matches Go: .css-1ugicyv-PaginationButton, .css-1j3fv0j-PaginationButton
    page_buttons = soup.select(".css-1ugicyv-PaginationButton, .css-1j3fv0j-PaginationButton")
    for btn in page_buttons:
        try:
            page_num = int(btn.get_text(strip=True))
            if page_num > pagination["totalPages"]:
                pagination["totalPages"] = page_num
        except ValueError:
            continue

    # 5. Search Metadata
    search_meta = {"query": "", "totalResults": 0}
    search_input = soup.select_one("input[data-testid='site-search']")
    if search_input:
        search_meta["query"] = search_input.get("value", "")
    
    confirm_text = soup.select_one("[data-testid='search-confirmation']")
    if confirm_text:
        res_text = confirm_text.get_text(strip=True)
        re_total = re.search(r'(\d[\d,]*)', res_text)
        if re_total:
            try:
                search_meta["totalResults"] = int(re_total.group(1).replace(",", ""))
            except ValueError:
                pass

    return ScrapedData(
        breadcrumbs=breadcrumbs,
        pagination=pagination,
        products=products,
        searchMetadata=search_meta
    )

def scrape_page(url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    """Scrape a single page with retry logic."""
    payload = {
        "api_key": API_KEY,
        "url": url,
        "optimize_request": True,
    }

    tries = 0
    success = False

    while tries <= retries and not success:
        try:
            proxy_url = 'https://proxy.scrapeops.io/v1/?' + urlencode(payload)
            response = requests.get(proxy_url, timeout=30)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, "lxml")
                scraped_data = extract_data(soup)
                
                if scraped_data and (scraped_data.products or scraped_data.breadcrumbs):
                    pipeline.add_data(scraped_data)
                    success = True
                else:
                    logger.warning(f"No meaningful data extracted from {url}")
            else:
                logger.warning(f"Request failed for {url} with status {response.status_code}")
        except Exception as e:
            logger.error(f"Exception scraping {url}: {e}")
        finally:
            tries += 1

    if not success:
        logger.error(f"Failed to scrape {url} after {retries} retries.")

def concurrent_scraping(urls: List[str], max_threads: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    """Scrape multiple URLs concurrently."""
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = [executor.submit(scrape_page, url, pipeline, max_retries) for url in urls]
        for future in futures:
            try:
                future.result()
            except Exception as e:
                logger.error(f"Thread execution error: {e}")

if __name__ == "__main__":
    urls = [
        "https://stockx.com/search?s=kids+shoes",
    ]

    logger.info("Starting concurrent scraping...")
    concurrent_scraping(urls, max_threads=1, max_retries=3)
    logger.info("Scraping complete.")