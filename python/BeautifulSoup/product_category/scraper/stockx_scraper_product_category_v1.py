"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-17
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urlencode, urljoin
import json
import re
import logging
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List
from concurrent.futures import ThreadPoolExecutor

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"stockx_com_product_category_page_scraper_data_{timestamp}.jsonl"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ScrapedData:
    categoryName: Optional[str] = None
    categoryUrl: Optional[str] = None
    description: Optional[str] = None
    appliedFilters: List[Dict[str, str]] = field(default_factory=list)
    breadcrumbs: List[Dict[str, str]] = field(default_factory=list)
    pagination: Dict[str, Any] = field(default_factory=dict)
    products: List[Dict[str, Any]] = field(default_factory=list)
    subcategories: List[Dict[str, Any]] = field(default_factory=list)
    bannerImage: Optional[str] = None
    categoryId: Optional[str] = None

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data: ScrapedData):
        item_key = input_data.categoryUrl or str(input_data.categoryName)
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found: {item_key}. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")

def strip_html(html_str: str) -> str:
    """Removes HTML tags from a string."""
    if not html_str:
        return ""
    clean = re.compile("<[^>]*>")
    return re.sub(clean, " ", html_str).strip()

def make_absolute_url(url_str: str) -> str:
    """Converts relative URLs to absolute StockX URLs."""
    if not url_str:
        return ""
    if url_str.startswith(("http://", "https://")):
        return url_str
    if url_str.startswith("//"):
        return f"https:{url_str}"
    domain = "https://stockx.com"
    return urljoin(domain, url_str)

def detect_currency(price_text: str) -> str:
    """Detects currency code from price text."""
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR",
        "GBP": "GBP", "£": "GBP", "GB£": "GBP",
        "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
        "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
        "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
        "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
        "CHF": "CHF", "FR.": "CHF",
        "SEK": "SEK", "KR": "SEK",
        "NZD": "NZD", "NZ$": "NZD",
    }
    for code, currency in currency_map.items():
        if code in price_text:
            return currency
    return "USD"

def extract_numeric_price(price_text: str) -> float:
    """Extracts float value from price string."""
    if not price_text:
        return 0.0
    clean_text = price_text.replace(",", "")
    match = re.search(r"\d+\.?\d*", clean_text)
    if match:
        try:
            return float(match.group())
        except ValueError:
            return 0.0
    return 0.0

def extract_data(soup: BeautifulSoup) -> Optional[ScrapedData]:
    """Extract structured data from HTML using BeautifulSoup."""
    try:
        next_data = {}
        next_data_script = soup.find("script", id="__NEXT_DATA__")
        if next_data_script:
            try:
                next_data = json.loads(next_data_script.string)
            except (json.JSONDecodeError, TypeError):
                pass

        data = ScrapedData()

        # Applied Filters
        filters = []
        filter_els = soup.select("#filters-container .css-8kxxxt, #filters-container div[data-testid='filter-chips'] button")
        for s in filter_els:
            name_el = s.select_one(".css-1zznpy")
            name = name_el.get_text(strip=True) if name_el else ""
            
            value_el = s.select_one(".css-m41fx8, .css-1ei05o8, span")
            value = value_el.get_text(strip=True) if value_el else s.get_text(strip=True)
            
            if name and value:
                filters.append({"filterName": name, "filterValue": value})
        data.appliedFilters = filters

        # Breadcrumbs
        breadcrumbs = []
        for li in soup.select("nav[data-testid='Breadcrumbs'] ol li"):
            name = li.get_text(strip=True)
            anchor = li.find("a")
            url = make_absolute_url(anchor.get("href", "")) if anchor else ""
            if name:
                breadcrumbs.append({"name": name, "url": url})
        data.breadcrumbs = breadcrumbs

        # Category Info
        cat_name_el = soup.select_one("h1.chakra-heading")
        cat_name = cat_name_el.get_text(strip=True) if cat_name_el else ""
        if not cat_name:
            title_el = soup.find("title")
            cat_name = title_el.get_text(strip=True) if title_el else ""
        data.categoryName = cat_name

        canonical = soup.find("link", rel="canonical")
        data.categoryUrl = make_absolute_url(canonical.get("href", "")) if canonical else ""

        desc_el = soup.select_one("p.chakra-text.css-2bkegz")
        raw_desc = desc_el.get_text(strip=True) if desc_el else ""
        data.description = strip_html(raw_desc)

        # Pagination logic
        pagination = {
            "currentPage": 1,
            "totalPages": 1,
            "totalResults": 0,
            "resultsPerPage": 40,
            "nextPageUrl": None,
            "prevPageUrl": None
        }

        # Safe nested navigation for NEXT_DATA
        try:
            browse_results = next_data.get("props", {}).get("pageProps", {}).get("states", {}).get("query", {}).get("value", {}).get("queries", [{}])[0].get("state", {}).get("data", {}).get("browse", {}).get("results", {})
            info = browse_results.get("pageInfo", {})
            if info:
                pagination["currentPage"] = info.get("page", 1)
                pagination["totalPages"] = info.get("pageCount", 1)
                pagination["totalResults"] = info.get("total", 0)
                pagination["resultsPerPage"] = info.get("limit", 40)
        except (IndexError, AttributeError, KeyError):
            pass

        next_page_el = soup.select_one("a[aria-label='Next']")
        if next_page_el and next_page_el.get("href"):
            pagination["nextPageUrl"] = make_absolute_url(next_page_el.get("href"))
        data.pagination = pagination

        # Products
        products = []
        product_tiles = soup.select("div[data-testid='ProductTile'], div[data-testid='VariantTile']")
        for s in product_tiles:
            p = {}
            name_el = s.select_one("p[data-testid='product-tile-title']")
            img_el = s.find("img")
            name = name_el.get_text(strip=True) if name_el else (img_el.get("alt", "") if img_el else "")
            p["name"] = name.strip()

            price_el = s.select_one("p[data-testid='product-tile-lowest-ask-amount']")
            price_str = price_el.get_text(strip=True) if price_el else ""
            p["price"] = extract_numeric_price(price_str)
            p["currency"] = detect_currency(price_str)
            p["availability"] = "in_stock"
            p["brand"] = "Nike"
            p["image"] = img_el.get("src") if img_el else None
            
            anchor = s.find("a")
            link = anchor.get("href", "") if anchor else ""
            p["url"] = make_absolute_url(link)
            
            p["isSponsored"] = bool(s.select_one("span[data-testid='sponsored-tag']")) or s.get("data-sponsored-listing") == "true"
            p["isPrime"] = False
            
            # Product ID Extraction logic
            product_id = s.get("data-product-id") or s.select_one("div[data-testid='productTile']").get("id", "") if s.select_one("div[data-testid='productTile']") else ""
            
            if not product_id and next_data:
                slug = ""
                if link:
                    slug = link.strip("/").split("/")[-1].split("?")[0]
                
                try:
                    queries = next_data.get("props", {}).get("pageProps", {}).get("states", {}).get("query", {}).get("value", {}).get("queries", [])
                    for q in queries:
                        edges = q.get("state", {}).get("data", {}).get("browse", {}).get("results", {}).get("edges", [])
                        for edge in edges:
                            node = edge.get("node", {})
                            if (slug and node.get("urlKey") == slug) or (p["name"] and node.get("name") == p["name"]):
                                product_id = node.get("id", "")
                                break
                        if product_id: break
                except Exception:
                    pass

            p["productId"] = product_id
            p["preDiscountPrice"] = None
            p["rating"] = None
            p["reviewCount"] = None

            if p["name"]:
                products.append(p)
        data.products = products

        # Subcategories
        subcats = []
        for s in soup.select("#filters-container .filter.css-qlswoo a"):
            name = s.get_text(strip=True)
            url = make_absolute_url(s.get("href", ""))
            if name:
                subcats.append({
                    "name": name,
                    "url": url,
                    "productCount": None
                })
        data.subcategories = subcats

        return data
    except Exception as e:
        logger.error(f"Error extracting data: {e}")
        return None

def scrape_page(url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    """Scrape a single page with retry logic."""
    payload = {
        "api_key": API_KEY,
        "url": url,
        "optimize_request": True,
    }

    tries = 0
    success = False

    while tries <= retries and not success:
        try:
            proxy_url = 'https://proxy.scrapeops.io/v1/?' + urlencode(payload)
            response = requests.get(proxy_url, timeout=30)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, "lxml")
                scraped_data = extract_data(soup)
                
                if scraped_data:
                    pipeline.add_data(scraped_data)
                    success = True
                else:
                    logger.warning(f"No data extracted from {url}")
            else:
                logger.warning(f"Request failed for {url} with status {response.status_code}")
        except Exception as e:
            logger.error(f"Exception scraping {url}: {e}")
        finally:
            tries += 1

    if not success:
        logger.error(f"Failed to scrape {url} after {retries} retries.")

def concurrent_scraping(urls: List[str], max_threads: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    """Scrape multiple URLs concurrently."""
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = [executor.submit(scrape_page, url, pipeline, max_retries) for url in urls]
        for future in futures:
            try:
                future.result()
            except Exception as e:
                logger.error(f"Thread generated an exception: {e}")

if __name__ == "__main__":
    urls = [
        "https://stockx.com/brands/nike?category=sneakers&gender=kids&kids-age-group=preschool",
    ]

    logger.info("Starting concurrent scraping...")
    concurrent_scraping(urls, max_threads=1, max_retries=3)
    logger.info("Scraping complete.")