"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-17
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urlencode, urlparse, parse_qs, urlunparse
import json
import re
from concurrent.futures import ThreadPoolExecutor
import logging
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"stockx_com_product_page_scraper_data_{timestamp}.jsonl"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ScrapedData:
    name: str = ""
    productId: str = ""
    brand: str = ""
    category: str = ""
    description: str = ""
    currency: str = "USD"
    price: float = 0.0
    preDiscountPrice: Optional[float] = None
    availability: str = "in_stock"
    url: str = ""
    images: List[Dict[str, str]] = field(default_factory=list)
    specifications: List[Dict[str, str]] = field(default_factory=list)
    serialNumbers: List[Dict[str, str]] = field(default_factory=list)
    seller: Dict[str, Any] = field(default_factory=lambda: {
        "name": "StockX",
        "rating": None,
        "url": "https://stockx.com"
    })
    aggregateRating: Optional[Any] = None
    features: List[str] = field(default_factory=list)
    reviews: List[Any] = field(default_factory=list)
    videos: List[Any] = field(default_factory=list)

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data: ScrapedData):
        item_key = input_data.url or input_data.productId or str(asdict(input_data))
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found: {item_key}. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")

def strip_html(html_str: str) -> str:
    """Removes HTML tags from a string."""
    if not html_str:
        return ""
    clean = re.compile("<[^>]*>")
    return re.sub(clean, " ", html_str).strip()

def make_absolute_url(url_str: str) -> str:
    """Converts relative URLs to absolute StockX URLs."""
    if not url_str:
        return ""
    if url_str.startswith(("http://", "https://")):
        return url_str
    if url_str.startswith("//"):
        return "https:" + url_str
    domain = "https://stockx.com"
    if url_str.startswith("/"):
        return domain + url_str
    return f"{domain}/{url_str}"

def detect_currency(price_text: str) -> str:
    """Detects currency code from price string."""
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR",
        "GBP": "GBP", "£": "GBP", "GB£": "GBP",
        "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
        "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
        "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
        "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
        "CHF": "CHF", "FR.": "CHF",
        "SEK": "SEK", "KR": "SEK",
        "NZD": "NZD", "NZ$": "NZD",
    }
    for code, currency in currency_map.items():
        if code in price_text:
            return currency
    return "USD"

def normalize_high_res(url_str: str) -> str:
    """Normalizes StockX image URLs to high resolution."""
    if "stockx.com/images" not in url_str:
        return url_str
    
    parsed = urlparse(url_str)
    params = parse_qs(parsed.query)
    
    # Override with high-res requirements
    new_params = {
        "fit": "fill",
        "bg": "FFFFFF",
        "w": "1200",
        "h": "857",
        "q": "60",
        "dpr": "1",
        "trim": "color"
    }
    
    # Keep other params except formatting ones
    for k, v in params.items():
        if k not in ["fm", "auto"] and k not in new_params:
            new_params[k] = v[0]
            
    query_string = urlencode(new_params)
    return urlunparse(parsed._replace(query=query_string))

def extract_data(soup: BeautifulSoup) -> Optional[ScrapedData]:
    """Extract structured data from HTML using BeautifulSoup."""
    try:
        # JSON-LD extraction
        json_ld = {}
        scripts = soup.select("script[type='application/ld+json']")
        for script in scripts:
            try:
                data = json.loads(script.get_text())
                if isinstance(data, dict) and data.get("@type") == "Product":
                    json_ld = data
                    break
            except json.JSONDecodeError:
                continue

        # __NEXT_DATA__ extraction
        next_data = {}
        next_script = soup.select_one("script#__NEXT_DATA__")
        if next_script:
            try:
                next_data = json.loads(next_script.get_text())
            except json.JSONDecodeError:
                pass

        data_obj = ScrapedData()

        # Basic Info
        data_obj.name = json_ld.get("name") or soup.select_one("[data-component='primary-product-title']").get_text(strip=True) if soup.select_one("[data-component='primary-product-title']") else ""
        data_obj.productId = json_ld.get("sku", "")
        data_obj.category = json_ld.get("category", "")
        
        # Brand extraction with fallback
        brand_name = ""
        if "brand" in json_ld and isinstance(json_ld["brand"], dict):
            brand_name = json_ld["brand"].get("name", "")
        
        if not brand_name:
            traits = soup.select("[data-component='ProductDetails'] [data-component='product-trait']")
            for trait in traits:
                if "Brand" in trait.get_text():
                    p_tag = trait.find("p")
                    if p_tag:
                        brand_name = p_tag.get_text(strip=True)
                        break
        data_obj.brand = brand_name

        # Description
        desc = json_ld.get("description", "")
        if not desc:
            desc_el = soup.select_one("[data-testid='product-description'] p")
            if desc_el:
                desc = desc_el.get_text()
        desc = desc.replace("<br>", "\n")
        data_obj.description = strip_html(desc)

        # Availability
        if json_ld.get("offers", {}).get("availability"):
            if "InStock" in json_ld["offers"]["availability"]:
                data_obj.availability = "in_stock"

        # Price and Currency
        # Target active section price: trade-box-buy-amount is usually unique to the active buy box
        price_el = soup.select_one("[data-testid='trade-box-buy-amount']")
        price_text = price_el.get_text(strip=True) if price_el else ""
        
        if price_text:
            data_obj.currency = detect_currency(price_text)
            match = re.search(r"[\d,.]+", price_text)
            if match:
                try:
                    data_obj.price = float(match.group().replace(",", ""))
                except ValueError:
                    pass
        elif next_data:
            # Fallback to localization data in NEXT_DATA
            try:
                data_obj.currency = next_data["props"]["pageProps"]["states"]["localization"]["currency"]
            except KeyError:
                pass
            
            if json_ld.get("offers", {}).get("lowPrice"):
                data_obj.price = float(json_ld["offers"]["lowPrice"])

        # Images
        seen_urls = set()
        final_images = []

        # Primary: Target NEXT_DATA gallery
        try:
            queries = next_data["props"]["pageProps"]["states"]["query"]["value"]["queries"]
            for q in queries:
                product_node = q.get("state", {}).get("data", {}).get("product", {})
                if product_node:
                    title = product_node.get("title", "")
                    # Verify it's related to our product
                    if not data_obj.name or data_obj.name in title or title in data_obj.name:
                        gallery = product_node.get("media", {}).get("gallery", [])
                        for i, img_url in enumerate(gallery):
                            high_res = normalize_high_res(img_url)
                            if high_res not in seen_urls:
                                seen_urls.add(high_res)
                                alt = data_obj.name if i == 0 else "nextProductImage"
                                final_images.append({"url": high_res, "alt_text": alt})
        except (KeyError, TypeError):
            pass

        # Fallback 1: HTML buttons
        if not final_images:
            img_buttons = soup.select("button.chakra-tabs__tab img")
            for i, img in enumerate(img_buttons):
                src = img.get("src")
                if src:
                    high_res = normalize_high_res(src)
                    if high_res not in seen_urls:
                        seen_urls.add(high_res)
                        alt = data_obj.name if i == 0 else "nextProductImage"
                        final_images.append({"url": high_res, "alt_text": alt})

        # Fallback 2: JSON-LD
        if not final_images and json_ld.get("image"):
            img_url = json_ld["image"]
            high_res = normalize_high_res(img_url)
            final_images.append({"url": high_res, "alt_text": data_obj.name})

        data_obj.images = final_images

        # Serial Numbers & Specs
        serials = []
        if data_obj.productId:
            serials.append({"type": "SKU", "value": data_obj.productId})
        
        if json_ld.get("gtin"):
            serials.append({"type": "GTIN", "value": str(json_ld["gtin"])})

        specs = []
        trait_elements = soup.select("[data-component='product-trait']")
        for trait in trait_elements:
            key_el = trait.find("span")
            val_el = trait.find("p")
            if key_el and val_el:
                key = key_el.get_text(strip=True)
                val = val_el.get_text(strip=True)
                if key and val:
                    specs.append({"key": key, "value": val})
                    if key == "Style":
                        serials.append({"type": "Other", "value": val})
        
        data_obj.specifications = specs
        data_obj.serialNumbers = serials

        # URL
        canonical = soup.find("link", rel="canonical")
        raw_url = json_ld.get("url") or (canonical.get("href") if canonical else "")
        data_obj.url = make_absolute_url(raw_url)

        return data_obj
    except Exception as e:
        logger.error(f"Error extracting data: {e}")
        return None

def scrape_page(url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    """Scrape a single page with retry logic."""
    payload = {
        "api_key": API_KEY,
        "url": url,
        "optimize_request": True,
        "render_js": True,
    }

    tries = 0
    success = False

    while tries <= retries and not success:
        try:
            proxy_url = 'https://proxy.scrapeops.io/v1/?' + urlencode(payload)
            response = requests.get(proxy_url, timeout=30)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, "lxml")
                scraped_data = extract_data(soup)
                
                if scraped_data:
                    pipeline.add_data(scraped_data)
                    success = True
                else:
                    logger.warning(f"No data extracted from {url}")
            else:
                logger.warning(f"Request failed for {url} with status {response.status_code}")
        except Exception as e:
            logger.error(f"Exception scraping {url}: {e}")
        finally:
            tries += 1

    if not success:
        logger.error(f"Failed to scrape {url} after {retries} retries.")

def concurrent_scraping(urls: List[str], max_threads: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    """Scrape multiple URLs concurrently."""
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = [executor.submit(scrape_page, url, pipeline, max_retries) for url in urls]
        for future in futures:
            future.result()

if __name__ == "__main__":
    urls = [
        "https://stockx.com/adidas-ctt-chinese-track-top-31-gender-neutral-jacket-asia-sizing-dark-grey",
    ]

    logger.info("Starting concurrent scraping...")
    concurrent_scraping(urls, max_threads=1, max_retries=3)
    logger.info("Scraping complete.")